{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3394304",
      "metadata": {},
      "source": [
        "Project notes placeholder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e3684c",
      "metadata": {},
      "source": [
        "# several common ways to build a Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b21dd31a",
      "metadata": {},
      "source": [
        "Here are **several common ways to build a Keras model**, from simplest to most flexible:\n",
        "\n",
        "---\n",
        "\n",
        "## 1) **Sequential API (stack of layers)**\n",
        "\n",
        "Best when your model is just a straight pipeline (no branches).\n",
        "\n",
        "```python\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\", input_shape=(20,)),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "```\n",
        "\n",
        "‚úÖ Simple & clean\n",
        "‚ùå Not good for multi-input / multi-output / skip connections\n",
        "\n",
        "---\n",
        "\n",
        "## 2) **Sequential API (add layers one by one)**\n",
        "\n",
        "Same as above, just built step-by-step.\n",
        "\n",
        "```python\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, activation=\"relu\", input_shape=(20,)))\n",
        "model.add(layers.Dense(32, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3) **Functional API (recommended for complex models)**\n",
        "\n",
        "Best for models with branching, multiple inputs/outputs, or skip connections.\n",
        "\n",
        "```python\n",
        "inputs = keras.Input(shape=(20,))\n",
        "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "```\n",
        "\n",
        "‚úÖ Flexible\n",
        "‚úÖ Supports multi-input/output\n",
        "‚úÖ Clean graph-style building\n",
        "\n",
        "---\n",
        "\n",
        "## 4) **Subclassing `keras.Model` (most customizable)**\n",
        "\n",
        "Best when you need full control (custom training logic, dynamic behavior).\n",
        "\n",
        "```python\n",
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = layers.Dense(64, activation=\"relu\")\n",
        "        self.d2 = layers.Dense(32, activation=\"relu\")\n",
        "        self.out = layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.d1(inputs)\n",
        "        x = self.d2(x)\n",
        "        return self.out(x)\n",
        "\n",
        "model = MyModel()\n",
        "```\n",
        "\n",
        "‚úÖ Maximum control\n",
        "‚ùå Slightly more code\n",
        "‚ùå Harder to visualize than Functional API\n",
        "\n",
        "---\n",
        "\n",
        "## 5) **Using the Keras `Model` with multiple inputs**\n",
        "\n",
        "Example: combining two different feature sets.\n",
        "\n",
        "```python\n",
        "input1 = keras.Input(shape=(10,))\n",
        "input2 = keras.Input(shape=(5,))\n",
        "\n",
        "x1 = layers.Dense(32, activation=\"relu\")(input1)\n",
        "x2 = layers.Dense(16, activation=\"relu\")(input2)\n",
        "\n",
        "x = layers.concatenate([x1, x2])\n",
        "output = layers.Dense(1)(x)\n",
        "\n",
        "model = keras.Model(inputs=[input1, input2], outputs=output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 6) **Model with shared layers (Siamese-style)**\n",
        "\n",
        "Same layer reused on multiple inputs.\n",
        "\n",
        "```python\n",
        "shared = layers.Dense(32, activation=\"relu\")\n",
        "\n",
        "input1 = keras.Input(shape=(20,))\n",
        "input2 = keras.Input(shape=(20,))\n",
        "\n",
        "x1 = shared(input1)\n",
        "x2 = shared(input2)\n",
        "\n",
        "merged = layers.concatenate([x1, x2])\n",
        "output = layers.Dense(1)(merged)\n",
        "\n",
        "model = keras.Model([input1, input2], output)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also show **CNN examples, RNN/LSTM examples, ResNet skip connections**, or **custom training loop** versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0728f321",
      "metadata": {},
      "source": [
        "# different loss functions you can use in `model.compile()` in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5004287",
      "metadata": {},
      "source": [
        "Here are **different loss functions you can use in `model.compile()` in Keras**, grouped by task üëá\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 1) Regression Losses (predicting numbers)\n",
        "\n",
        "### **Mean Squared Error (MSE)**\n",
        "\n",
        "```python\n",
        "loss=\"mse\"   # or \"mean_squared_error\"\n",
        "```\n",
        "\n",
        "### **Mean Absolute Error (MAE)**\n",
        "\n",
        "```python\n",
        "loss=\"mae\"   # or \"mean_absolute_error\"\n",
        "```\n",
        "\n",
        "### **Huber Loss (robust to outliers)**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.losses import Huber\n",
        "loss=Huber()\n",
        "```\n",
        "\n",
        "### **Mean Squared Log Error (MSLE)**\n",
        "\n",
        "Good when targets grow exponentially\n",
        "\n",
        "```python\n",
        "loss=\"msle\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 2) Binary Classification Losses (2 classes)\n",
        "\n",
        "### **Binary Crossentropy (most common)**\n",
        "\n",
        "```python\n",
        "loss=\"binary_crossentropy\"\n",
        "```\n",
        "\n",
        "‚úÖ Use with:\n",
        "\n",
        "* output layer: `Dense(1, activation=\"sigmoid\")`\n",
        "\n",
        "### **Binary Crossentropy (logits version)**\n",
        "\n",
        "If your model outputs raw values (no sigmoid)\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "loss=BinaryCrossentropy(from_logits=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 3) Multi-class Classification Losses (more than 2 classes)\n",
        "\n",
        "### **Categorical Crossentropy**\n",
        "\n",
        "Use when labels are **one-hot encoded**\n",
        "\n",
        "```python\n",
        "loss=\"categorical_crossentropy\"\n",
        "```\n",
        "\n",
        "‚úÖ Output:\n",
        "\n",
        "* `Dense(num_classes, activation=\"softmax\")`\n",
        "\n",
        "### **Sparse Categorical Crossentropy**\n",
        "\n",
        "Use when labels are **integer encoded**\n",
        "\n",
        "```python\n",
        "loss=\"sparse_categorical_crossentropy\"\n",
        "```\n",
        "\n",
        "### **Sparse Categorical Crossentropy (logits)**\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "loss=SparseCategoricalCrossentropy(from_logits=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 4) Multi-label Classification Losses\n",
        "\n",
        "(Each sample can belong to multiple classes)\n",
        "\n",
        "```python\n",
        "loss=\"binary_crossentropy\"\n",
        "```\n",
        "\n",
        "‚úÖ Output:\n",
        "\n",
        "* `Dense(num_labels, activation=\"sigmoid\")`\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 5) Special / Advanced Losses\n",
        "\n",
        "### **KLDivergence**\n",
        "\n",
        "Often used in probability distribution tasks / VAEs\n",
        "\n",
        "```python\n",
        "loss=\"kullback_leibler_divergence\"\n",
        "```\n",
        "\n",
        "### **Poisson Loss**\n",
        "\n",
        "Used for count prediction\n",
        "\n",
        "```python\n",
        "loss=\"poisson\"\n",
        "```\n",
        "\n",
        "### **Cosine Similarity Loss**\n",
        "\n",
        "Used in embeddings / similarity tasks\n",
        "\n",
        "```python\n",
        "loss=\"cosine_similarity\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Example usage in compile()\n",
        "\n",
        "### Binary classification\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "### Regression\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"mse\",\n",
        "              metrics=[\"mae\"])\n",
        "```\n",
        "\n",
        "### Multi-class with integer labels\n",
        "\n",
        "```python\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "If you tell me your **problem type (regression / binary / multiclass / multilabel)** and your **output layer**, I can recommend the *best* loss function for it ‚úÖ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
